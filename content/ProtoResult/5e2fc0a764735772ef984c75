ProtoResult:
. "Let ${ \/}$ be the transition matrix for a regular Markov 
   chain with fixed vector ${ \/}$. Then for any initial 
   probability vector ${ \/}$, ${ \/}^n arrow { \/}$ as $n 
   arrow \infty$ .2in {\bf Proof.\ } Let $ X_0,\ X_1,\ \ldots$ 
   be a Markov chain with transition matrix ${ \/}$ started in 
   state $s_i$. Let $Y_0,\ Y_1,\ \ldots$ be a Markov chain with
   transition probability ${ \/}$ started with initial 
   probabilities given by ${ \/}$. The $X$ and $Y$ processes 
   are run independently of each other. \par We consider also a
   third Markov chain ${ \/}^*$ which consists of watching both
   the $X$ and $Y$ processes. The states for ${ \/}^*$ are 
   pairs $(s_i, s_j)$. The transition probabilities are given 
   by $${ \/}^{*}[(i,j),(k,l)] = { \/}(i,k) \cdot { \/}(j,l)\ 
   $$ Since ${ \/}$ is regular there is an $N$ such that ${ 
   \/}^{N}(i,j) > 0$ for all $i$ and $j$. Thus for the ${ 
   \/}^*$ chain it is also possible to go from any state $(s_i,
   s_j)$ to any other state $(s_k,s_l)$ in at most $N$ steps. 
   That is ${ \/}^*$ is also a regular Markov chain. \par We 
   know that a regular Markov chain will reach any state in a 
   finite time. Let $T$ be the first time the the chain ${ 
   \/}^*$ is in a state of the form $(s_k,s_k)$. In other 
   words, $T$ is the first time that the $X$ and the $Y$ 
   processes are in the same state. Then we have shown that $$ 
   P[T > n] arrow 0 n arrow \infty\ . $$ If we watch the $X$ 
   and $Y$ processes after the first time they are in the same 
   state we would not predict any difference in their long 
   range behavior. Since this will happen no matter how we 
   started these two processes, it seems clear that the long 
   range behaviour should not depend upon the starting state. 
   We now show that this is true. \par We first note that if $n
   \ge T$, then since $X$ and $Y$ are both in the same state at
   time $T$, $$ P(X_n = j\ |\ n \ge T) = P(Y_n = j\ |\ n \ge 
   T)\ . $$ If we multiply both sides of this equation by $P(n 
   \ge T)$, we obtain $ P(X_n = j,\ n \ge T) = P(Y_n = j,\ n 
   \ge T)\ . $ We know that for all $n$, $$P(Y_n = j) = w_j\ $$
   But $$P(Y_n = j) = P(Y_n = j,\ n \ge T) + P(Y_n = j,\ n < 
   T)\ $$ and the second summand on the right-hand side of this
   equation goes to 0 as $n$ goes to $\infty$, since $P(n < T)$
   goes to 0 as $n$ goes to $\infty$. So, $$P(Y_n = j,\ n \ge 
   T) arrow w_j\ $$ as $n$ goes to $\infty$. From Equation 
   11.4.1 , we see that $$P(X_n = j,\ n \ge T) arrow w_j\ $$ as
   $n$ goes to $\infty$. But by similar reasoning to that used 
   above, the difference between this last expression and 
   $P(X_n = j)$ goes to 0 as $n$ goes to $\infty$. Therefore, 
   $$P(X_n = j) arrow w_j\ $$ as $n$ goes to $\infty$. This 
   completes the proof."
Metadata:
. reference:
  . source: "@IntroductionToProbability"
    page: "451"
    offset: "461"
. id: "5e2fc0a764735772ef984c75"

