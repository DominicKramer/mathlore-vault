ProtoResult:
. "{\bf (Central Limit Theorem)} Let $X_1,\ X_2,\ \ldots,\ 
   X_n\ ,\ \ldots$ be a sequence of independent discrete random
   variables, and let $S_n = X_1 + X_2 +\cdots+ X_n$. For each 
   $n$, denote the mean and variance of $X_n$ by $\mu_n$ and 
   $\sigma^2_n$, respectively. Define the mean and variance of 
   $S_n$ to be $m_n$ and $s_n^2$, respectively, and assume that
   $s_n arrow \infty$. If there exists a constant $A$, such 
   that $|X_n| \le A$ for all $n$, then for $a < b$, $$ \lim_{n
   \to \infty} P ( a < \frac {S_n - m_n}{s_n} < b ) = \frac 
   1{\sqrt{2\pi}} \int_a^b e^{-x^2/2} dx\ . $$"
Metadata:
. reference:
  . source: "@IntroductionToProbability"
    page: "347"
    offset: "357"
. id: "5e2fc0a764735772ef984c77"

